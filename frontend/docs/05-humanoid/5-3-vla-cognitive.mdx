---
id: lesson-5-3-vla-cognitive
title: Conversational Robotics (VLA) - GPT to ROS 2 Action Sequence
sidebar_label: VLA Cognitive
chapter_id: 05
page_number: 20
slug: /05-humanoid/5-3-vla-cognitive
---

# Lesson 5.3: Conversational Robotics (VLA): GPT to ROS 2 Action Sequence

This lesson covers **Vision Language Models (VLMs)** and how large cognitive models translate **ambiguous human commands** into precise sequences of robotic actions.

---

## Introduction & Hook

- We've learned how a robot:
  - Stands: **ZMP** (Lesson 5.1)  
  - Sees: **Cameras/LIDAR** (Lesson 1.3)  
  - Picks objects: **Manipulation** (Lesson 5.2)  

- But how does it **decide what to do**?  

Example: Command: `"Please put the tools away."`  

Required reasoning:
1. **Perception:** Identify "tools."  
2. **Context:** Identify "away" (the toolbox).  
3. **Planning:** Generate a sequence of 50 steps: walk, reach, grasp, turn, release.  

- **VLMs** bridge human language and robot action.

---

## Vision Language Models (VLMs)

- A **VLM** is an advanced Large Language Model (LLM) that processes **text and visual input**.  
- Provides **situational awareness** for high-level tasks.

### Role in Robotics

- Acts as a **Cognitive Engine** / Mission Planner.  
- Operates at **low frequency** compared to real-time control loops.  

Functions:
1. **Reasoning:** Parse language command with visual context.  
2. **Grounding:** Map language to physical world coordinates.  
3. **Task Decomposition:** Break high-level command into sequential sub-tasks.

---

## LLM-to-Action Sequence Pipeline

Transforms human commands into **ROS 2 actions**:

1. **Natural Language Input**  
   `"Grab the wrench next to the red safety box."`

2. **LLM Parsing & Goal Specification**  
   Converts sentence into structured data (JSON/YAML):

```json
{
  "goal": "pick_and_place",
  "target_object": "wrench",
  "target_location": "near red safety box"
}
````

3. **Cognitive Planner**
   Retrieves sub-tasks from the robot’s library:

* Sub-Task 1: `nav_to_goal` (Locomotion)
* Sub-Task 2: `run_vision_detect` (Perception)
* Sub-Task 3: `execute_grasp` (Manipulation)

4. **ROS 2 Action Sequence Generation**
   Planner outputs step-by-step **ROS 2 Topics/Actions**:

> Human Language -> VLM Reasoning -> Structured Goal -> ROS 2 Commands

---

## Brainstorming Challenge: Ambiguity in Human Language

* Task: A user says `"Please tidy up."`
* Questions:

  1. Why is this impossible without a VLM?
  2. If clarified as `"Put the book in the bag,"` what **coordinate data** does the VLM need from camera/LIDAR (Lesson 1.3) to complete the command?

---

## Key Concepts in Action

| Concept                         | Function                                   | Example                                                           |
| :------------------------------ | :----------------------------------------- | :---------------------------------------------------------------- |
| **Vision Language Model (VLM)** | Processes text and visual data             | Translates "blue cube" into exact $x, y, z$ coordinates           |
| **Cognitive Engine**            | Handles task decomposition                 | Breaks "make coffee" into "get mug," "insert pod," "press button" |
| **Grounding**                   | Maps language concepts to physical objects | Confirms "red door" is present in camera image                    |
| **Action Sequence**             | Step-by-step ROS 2 commands                | `move_arm {joint_angles: ...}`, `set_gripper {force: 10N}`        |

---

## Self-Test Q&A

**Q1:** What is the primary advantage of a VLM over a standard LLM in robotics?
**A1:** A VLM can process visual input, grounding language commands in the physical environment.

**Q2:** What is the technical term for breaking a high-level command into smaller steps?
**A2:** **Task Decomposition**.

**Q3:** Which data formats are typically used to represent goals before planning?
**A3:** **JSON** or **YAML**.

**Q4:** In the pipeline, what does the Cognitive Planner retrieve to build ROS 2 commands?
**A4:** Sub-tasks from the robot’s library (e.g., `nav_to_goal`, `execute_grasp`).

**Q5:** What is the term for mapping an abstract concept to a physical object with coordinates?
**A5:** **Grounding**.

